{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rNuohp44N00i"
   },
   "source": [
    "## Objetivos\n",
    "\n",
    "Usaremos Python para implementar varios algoritmos de reinforcement learning.\n",
    "\n",
    "Ejecutaremos los algoritmos en algunos problemas para entender las propiedades y diferentes comportamientos emergentes. \n",
    "En este tutorial pondremos el foco en los fundamentos de RL en un simple gridworld."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztQEQvnKh2t6"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ps5OnkPmDbMX"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import itertools\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=1)\n",
    "plt.style.use('seaborn-notebook')\n",
    "plt.style.use('seaborn-whitegrid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ALrRR76eAd6u"
   },
   "source": [
    "## Environments: Grid-Worlds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uMC6nODK1HAV"
   },
   "source": [
    "**(Simple) Tabular Grid-World**\n",
    "\n",
    "Puedes visualizar el mundo grilla en donde entrenaremos nuestro agente (ejecutando la celda siguiente).\n",
    "\n",
    "`S` indica el estado inicial y `G` indica el objetivo.  El agente tiene cuatro acciones posibles: arriba, derecha, abajo, e izquierda.  Rewards es: `-5` si salimos fuera de las paredes, `+10` si alcanzamos el objetivo, y `0` en otro caso.  El episodio termina cuando el agente alcance el objetivo. El descuento es $\\gamma = 0.9$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "YP97bVN3NuG8"
   },
   "outputs": [],
   "source": [
    "#@title Environment: Gridworld Implementation\n",
    "class Grid(object):\n",
    "\n",
    "  def __init__(self, discount=0.9, penalty_for_walls=-5):\n",
    "    # -1: wall\n",
    "    # 0: empty, episode continues\n",
    "    # other: number indicates reward, episode will terminate\n",
    "    self._layout = np.array([\n",
    "      [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
    "      [-1,  0,  0,  0,  0,  0, -1,  0,  0, -1],\n",
    "      [-1,  0,  0,  0, -1,  0,  0,  0, 10, -1],\n",
    "      [-1,  0,  0,  0, -1, -1,  0,  0,  0, -1],\n",
    "      [-1,  0,  0,  0, -1, -1,  0,  0,  0, -1],\n",
    "      [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
    "      [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
    "      [-1,  0,  0,  0,  0,  0,  0,  0,  0, -1],\n",
    "      [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
    "    ])\n",
    "    self._start_state = (2, 2)\n",
    "    self._goal_state = (8, 2)\n",
    "    self._state = self._start_state\n",
    "    self._number_of_states = np.prod(np.shape(self._layout))\n",
    "    self._discount = discount\n",
    "    self._penalty_for_walls = penalty_for_walls\n",
    "    self._layout_dims = self._layout.shape\n",
    "\n",
    "  @property\n",
    "  def number_of_states(self):\n",
    "      return self._number_of_states\n",
    "    \n",
    "  def plot_grid(self):\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(self._layout <= -1, interpolation=\"nearest\")     \n",
    "    ax = plt.gca()\n",
    "    ax.grid(0)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"The grid\")\n",
    "    plt.text(\n",
    "        self._start_state[0], self._start_state[1], \n",
    "        r\"$\\mathbf{S}$\", ha='center', va='center')\n",
    "    plt.text(\n",
    "        self._goal_state[0], self._goal_state[1], \n",
    "        r\"$\\mathbf{G}$\", ha='center', va='center')\n",
    "    h, w = self._layout.shape\n",
    "    for y in range(h-1):\n",
    "      plt.plot([-0.5, w-0.5], [y+0.5, y+0.5], '-k', lw=2)\n",
    "    for x in range(w-1):\n",
    "      plt.plot([x+0.5, x+0.5], [-0.5, h-0.5], '-k', lw=2)\n",
    "\n",
    "  \n",
    "  def get_obs(self):\n",
    "    y, x = self._state\n",
    "    return y*self._layout.shape[1] + x\n",
    "  \n",
    "  def int_to_state(self, int_obs):\n",
    "    x = int_obs % self._layout.shape[1]\n",
    "    y = int_obs // self._layout.shape[1]\n",
    "    return y, x\n",
    "\n",
    "  def step(self, action):\n",
    "    y, x = self._state\n",
    "\n",
    "    if action == 0:  # up\n",
    "      new_state = (y - 1, x)\n",
    "    elif action == 1:  # right\n",
    "      new_state = (y, x + 1)\n",
    "    elif action == 2:  # down\n",
    "      new_state = (y + 1, x)\n",
    "    elif action == 3:  # left\n",
    "      new_state = (y, x - 1)\n",
    "    else:\n",
    "      raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
    "\n",
    "    new_y, new_x = new_state\n",
    "    if self._layout[new_y, new_x] == -1:  # wall\n",
    "      reward = self._penalty_for_walls\n",
    "      discount = self._discount\n",
    "      new_state = (y, x)\n",
    "    elif self._layout[new_y, new_x] == 0:  # empty cell\n",
    "      reward = 0.\n",
    "      discount = self._discount\n",
    "    else:  # a goal\n",
    "      reward = self._layout[new_y, new_x]\n",
    "      discount = 0.\n",
    "      new_state = self._start_state\n",
    "    \n",
    "    self._state = new_state\n",
    "    return reward, discount, self.get_obs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "ZVUhh2qqwep_",
    "outputId": "9273e0a5-b3b7-4b19-d703-900d3faf52f6"
   },
   "outputs": [],
   "source": [
    "# Visualise the environment\n",
    "\n",
    "# Instantiate the tabular environment\n",
    "grid = Grid()\n",
    "\n",
    "# Plot tabular environment\n",
    "grid.plot_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "rbEydMqDKxZr"
   },
   "outputs": [],
   "source": [
    "#@title Policies (Uniformly random and e-greedy) \n",
    "#Expected syntax: `policy(q_values)` \n",
    "\n",
    "# uniformly random policy\n",
    "def random_policy(q):\n",
    "  return np.random.randint(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cOu9RZY3AkF1"
   },
   "source": [
    "## Funciones de ayuda (para visualizar y ejecutar los experimentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "6EttQGJ1n5Zn"
   },
   "outputs": [],
   "source": [
    "#@title Helper functions for visualisation\n",
    "\n",
    "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
    "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
    "\n",
    "def plot_values(values, colormap='pink', vmin=-1, vmax=10):\n",
    "  plt.imshow(values, interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
    "  plt.yticks([])\n",
    "  plt.xticks([])\n",
    "  plt.colorbar(ticks=[vmin, vmax])\n",
    "\n",
    "def plot_state_value(action_values):\n",
    "  q = action_values\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "  vmin = np.min(action_values)\n",
    "  vmax = np.max(action_values)\n",
    "  v = 0.9 * np.max(q, axis=-1) + 0.1 * np.mean(q, axis=-1)\n",
    "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
    "  plt.title(\"$v(s)$\")\n",
    "\n",
    "def plot_action_values(action_values):\n",
    "  q = action_values\n",
    "  fig = plt.figure(figsize=(8, 8))\n",
    "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "  vmin = np.min(action_values)\n",
    "  vmax = np.max(action_values)\n",
    "  dif = vmax - vmin\n",
    "  for a in [0, 1, 2, 3]:\n",
    "    plt.subplot(3, 3, map_from_action_to_subplot(a))\n",
    "    \n",
    "    plot_values(q[..., a], vmin=vmin - 0.05*dif, vmax=vmax + 0.05*dif)\n",
    "    action_name = map_from_action_to_name(a)\n",
    "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
    "    \n",
    "  plt.subplot(3, 3, 5)\n",
    "  v = 0.9 * np.max(q, axis=-1) + 0.1 * np.mean(q, axis=-1)\n",
    "  plot_values(v, colormap='summer', vmin=vmin, vmax=vmax)\n",
    "  plt.title(\"$v(s)$\")\n",
    "      \n",
    "  \n",
    "def smooth(x, window=10):\n",
    "  return x[:window*(len(x)//window)].reshape(len(x)//window, window).mean(axis=1)\n",
    "  \n",
    "\n",
    "def plot_stats(stats, window=10):\n",
    "  plt.figure(figsize=(16,4))\n",
    "  plt.subplot(121)\n",
    "  xline = range(0, len(stats.episode_lengths), window)\n",
    "  plt.plot(xline, smooth(stats.episode_lengths, window=window))\n",
    "  plt.ylabel('Episode Length')\n",
    "  plt.xlabel('Episode Count')\n",
    "  plt.subplot(122)\n",
    "  plt.plot(xline, smooth(stats.episode_rewards, window=window))\n",
    "  plt.ylabel('Episode Return')\n",
    "  plt.xlabel('Episode Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U86SLdA25GAY"
   },
   "outputs": [],
   "source": [
    "#@title [IMPORTANT] Ejecuta los experimentos\n",
    "\n",
    "# Loop de interacción simple con el MDP:\n",
    "# 1) Interactíaa con el entorno\n",
    "# 2) El agente obtiene observación, recomensa y descuento del entorno. \n",
    "# y produce la siguiente acción\n",
    "def run_experiment(env, agent, number_of_steps):\n",
    "    mean_reward = 0.\n",
    "    try:\n",
    "      action = agent.initial_action()\n",
    "    except AttributeError:\n",
    "      action = 0\n",
    "      \n",
    "    # Interaction wih the MDP\n",
    "    for i in range(number_of_steps):\n",
    "      reward, discount, next_state = env.step(action)\n",
    "      action = agent.step(reward, discount, next_state)\n",
    "      mean_reward += (reward - mean_reward)/(i + 1.)\n",
    "\n",
    "    return mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "aWmvMHR5gM6N"
   },
   "outputs": [],
   "source": [
    "#@title Funciones para visualizar las políticas\n",
    "def plot_policy(grid, policy):\n",
    "  action_names = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "  grid.plot_grid()\n",
    "  plt.title('Policy Visualization')\n",
    "  for i in range(9):\n",
    "    for j in range(10):\n",
    "      action_name = action_names[policy[i,j]]\n",
    "      plt.text(j, i, action_name, ha='center', va='center')\n",
    "\n",
    "def plot_greedy_policy(grid, q):\n",
    "  action_names = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "  greedy_actions = np.argmax(q, axis=2)\n",
    "  grid.plot_grid()\n",
    "  plt.title('Greedy Policy')\n",
    "  for i in range(9):\n",
    "    for j in range(10):\n",
    "      action_name = action_names[greedy_actions[i,j]]\n",
    "      plt.text(j, i, action_name, ha='center', va='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5XEP4mf4Jx70"
   },
   "source": [
    "# Entrenar nuestro agente\n",
    "\n",
    "Cada agente tiene una función step:\n",
    "\n",
    "### `__init__(self, number_of_actions, number_of_states, initial_observation)`:\n",
    "El constructor dará al agente el número de acciones, número de estados y la observación inicial. \n",
    "\n",
    "### `step(self, reward, discount, next_observation, ...)`:\n",
    "Los pasos deberá actualizar los valores internos del agente y retornar la siguiente acción a tomar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de la política\n",
    "\n",
    "Primero vamos a evaluar una política $\\pi$\n",
    "\n",
    "\n",
    "Algoritmo:\n",
    "\n",
    "**Inicializar** $Q(s, a)$ para todo s ∈ $\\mathcal{S}$ y a ∈ $\\mathcal{A}(s)$\n",
    "\n",
    "**Loop forever**:\n",
    "\n",
    "1. $S \\gets{}$actual estado no terminal\n",
    " \n",
    "2. $A \\gets{} \\text{behaviour_policy}(S)$\n",
    " \n",
    "3. Toma acción $A$; observa recomensa $R$, descuento $\\gamma$, y estado, $S'$\n",
    "\n",
    "4. $Q(S, A) \\gets Q(S, A) + \\alpha (R + \\gamma Q(S', \\pi(S')) − Q(S, A))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nORJvcHML9os"
   },
   "outputs": [],
   "source": [
    "# uniformly random policy\n",
    "def random_policy(q):\n",
    "  return np.random.randint(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_IWIHIvxyC-H"
   },
   "outputs": [],
   "source": [
    "#@title [Coding Task] Policy Evaluation AGENT\n",
    "class PolicyEval_AGENT(object):\n",
    "\n",
    "  def __init__(\n",
    "      self, number_of_states, number_of_actions, initial_state, evaluated_policy, \n",
    "      behaviour_policy=random_policy, step_size=0.1):\n",
    "    self._action = 0\n",
    "    self._state = initial_state\n",
    "    self._number_of_states = number_of_states\n",
    "    self._number_of_actions = number_of_actions\n",
    "    self._step_size = step_size\n",
    "    self._behaviour_policy = behaviour_policy\n",
    "    self._evaluated_policy = evaluated_policy\n",
    "    \n",
    "    # initialize  q-values\n",
    "    self._q = np.zeros((number_of_states, number_of_actions))\n",
    "    \n",
    "  @property\n",
    "  def q_values(self):\n",
    "    return self._q\n",
    "\n",
    "  def step(self, reward, discount, next_state):\n",
    "    s = self._state\n",
    "    a = self._action\n",
    "    r = reward\n",
    "    d = discount\n",
    "    next_s = next_state\n",
    "    \n",
    "    # Q-value table update\n",
    "    qsa_next = self._q[next_s][self._evaluated_policy(self._q[next_s])]\n",
    "    alfa = self._step_size\n",
    "    qsa = self._q[s][a]\n",
    "    self._q[s][a] = self._q[s][a] + alfa*(r + d*qsa_next - self._q[s][a])\n",
    "  \n",
    "    # Get the action to send to execute in the environment and return it\n",
    "    self._state = next_s\n",
    "    self._action = self._behaviour_policy(self._q[self._state])\n",
    "  \n",
    "    return self._action\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TCPr9KzBtFJ3"
   },
   "source": [
    "**Pruébalo!** Ejecuta la evaluación de la política del agente, evaluando una política uniformemente aleatoria sobre el entorno Grid, probar con $\\texttt{num_steps} = 1e3, 1e5, 1e7$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "GZS4CfLtzaiX",
    "outputId": "4a586a89-7d21-49c1-8e50-80dd0d56cc13"
   },
   "outputs": [],
   "source": [
    "num_steps = int(1e5) # @param\n",
    "\n",
    "grid = Grid()\n",
    "\n",
    "agent = PolicyEval_AGENT(\n",
    "    number_of_states=grid._layout.size, \n",
    "    number_of_actions=4, \n",
    "    initial_state=grid.get_obs(),\n",
    "    evaluated_policy=random_policy,\n",
    "    behaviour_policy=random_policy,\n",
    "    step_size=0.1)\n",
    "\n",
    "# run experiment and get the value functions from agent\n",
    "run_experiment(grid, agent, num_steps)\n",
    "q = agent.q_values.reshape(grid._layout.shape + (4,))\n",
    "\n",
    "# visualise value functions\n",
    "plot_action_values(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obteniendo política Greedy (mejoramiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "i1cC9EZJzaia",
    "outputId": "580ddb46-dc95-4c85-8db9-73f3abbd44ed"
   },
   "outputs": [],
   "source": [
    "# visualise the greedy policy\n",
    "plot_greedy_policy(grid, q)\n",
    "\n",
    "# Show start action.\n",
    "start_action = np.argmax(q[grid._start_state])\n",
    "if start_action == 0:\n",
    "  start_action = \"Up\"\n",
    "elif start_action == 1:\n",
    "  start_action = \"Right\"\n",
    "elif start_action == 2:\n",
    "  start_action = \"Down\"\n",
    "else:\n",
    "  start_action = \"Left\"\n",
    "print(\"\\nAction from start state: \"+start_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "9v_SYckYfv5G",
    "rNuohp44N00i",
    "B8oKd0oyvNcH",
    "iJastp_kcAZC",
    "SfIkMhcDupVN",
    "k10bD7O9ujp4"
   ],
   "name": "Copia de DRL2019_Assignment2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
